{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"WARNING: Logging before flag parsing goes to stderr.\nW1019 16:06:47.609210 12224 deprecation.py:323] From C:\\Users\\Philip\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\nInstructions for updating:\nnon-resource variables are not supported in the long term\n"}],"source":"\n\n# To support both python 2 and python 3\nfrom __future__ import division, print_function, unicode_literals\nfrom io import open\n\n# Common imports\nimport numpy as np\nimport os\n\n# to make this notebook's output stable across runs\ndef reset_graph(seed=42):\n    tf.reset_default_graph()\n    tf.set_random_seed(seed)\n    np.random.seed(seed)\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12\n\n# Where to save the figures\nPROJECT_ROOT_DIR = \".\"\nCHAPTER_ID = \"cnn\"\n\ndef save_fig(fig_id, tight_layout=True):\n    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n    print(\"Saving figure\", fig_id)\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format='png', dpi=300)\n\n    \ndef plot_image(image):\n    plt.imshow(image, cmap=\"gray\", interpolation=\"nearest\")\n    plt.axis(\"off\")\n\ndef plot_color_image(image):\n    plt.imshow(image.astype(np.uint8),interpolation=\"nearest\")\n    plt.axis(\"off\")\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n\nreset_graph = ops.reset_default_graph\n\nfrom scipy.ndimage.interpolation import shift, zoom\nfrom datetime import datetime\nimport progressbar"},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"W1019 16:06:51.013635 12224 deprecation.py:323] From <ipython-input-2-675495a953e6>:30: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.keras.layers.Conv2D` instead.\nW1019 16:06:51.014635 12224 deprecation.py:323] From C:\\Users\\Philip\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\layers\\convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `layer.__call__` method instead.\nW1019 16:06:51.046643 12224 deprecation.py:323] From <ipython-input-2-675495a953e6>:40: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.Dense instead.\n"}],"source":"height = 28\nwidth = 28\nchannels = 1\nn_inputs = height * width\n\nconv1_fmaps = 32\nconv1_ksize = 3\nconv1_stride = 1\nconv1_pad = \"SAME\"\n\nconv2_fmaps = 64\nconv2_ksize = 3\nconv2_stride = 2\nconv2_pad = \"SAME\"\n\npool3_fmaps = conv2_fmaps\n\nn_fc1 = 64\nn_outputs = 10\n\nreset_graph()\n\nwith tf.name_scope(\"inputs\"):\n    X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n    X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])\n    y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n\nconv1 = tf.layers.conv2d(X_reshaped, filters=conv1_fmaps, kernel_size=conv1_ksize,\n                         strides=conv1_stride, padding=conv1_pad,\n                         activation=tf.nn.relu, name=\"conv1\")\nconv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n                         strides=conv2_stride, padding=conv2_pad,\n                         activation=tf.nn.relu, name=\"conv2\")\n\nwith tf.name_scope(\"pool3\"):\n    pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n    pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * 7 * 7])\n\nwith tf.name_scope(\"fc1\"):\n    fc1 = tf.layers.dense(pool3_flat, n_fc1, activation=tf.nn.relu, name=\"fc1\")\n\nwith tf.name_scope(\"output\"):\n    logits = tf.layers.dense(fc1, n_outputs, name=\"output\")\n    Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n\nwith tf.name_scope(\"train\"):\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n    loss = tf.reduce_mean(xentropy)\n    optimizer = tf.train.AdamOptimizer()\n    training_op = optimizer.minimize(loss)\n\nwith tf.name_scope(\"eval\"):\n    correct = tf.nn.in_top_k(logits, y, 1)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n\nwith tf.name_scope(\"init_and_save\"):\n    init = tf.global_variables_initializer()\n    saver = tf.train.Saver()"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":"(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\nX_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\nX_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\ny_train = y_train.astype(np.int32)\ny_test = y_test.astype(np.int32)\nX_valid, X_train = X_train[:5000], X_train[5000:]\ny_valid, y_train = y_train[:5000], y_train[5000:]"},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":"# Shift image by dx and xy\ndef shift_image(image, dx, dy):\n    image = image.reshape((28, 28))\n    shifted_image = shift(image, [dy, dx], cval=0, mode=\"constant\")\n    return shifted_image.reshape([-1])\n\ndef clipped_zoom(img, zoom_factor, **kwargs):\n\n    h, w = img.shape[:2]\n\n    # For multichannel images we don't want to apply the zoom factor to the RGB\n    # dimension, so instead we create a tuple of zoom factors, one per array\n    # dimension, with 1's for any trailing dimensions after the width and height.\n    zoom_tuple = (zoom_factor,) * 2 + (1,) * (img.ndim - 2)\n\n    # Zooming out\n    if zoom_factor < 1:\n\n        # Bounding box of the zoomed-out image within the output array\n        zh = int(np.round(h * zoom_factor))\n        zw = int(np.round(w * zoom_factor))\n        top = (h - zh) // 2\n        left = (w - zw) // 2\n\n        # Zero-padding\n        out = np.zeros_like(img)\n        out[top:top+zh, left:left+zw] = zoom(img, zoom_tuple, **kwargs)\n\n    # Zooming in\n    elif zoom_factor > 1:\n\n        # Bounding box of the zoomed-in region within the input array\n        zh = int(np.round(h / zoom_factor))\n        zw = int(np.round(w / zoom_factor))\n        top = (h - zh) // 2\n        left = (w - zw) // 2\n\n        out = zoom(img[top:top+zh, left:left+zw], zoom_tuple, **kwargs)\n\n        # `out` might still be slightly larger than `img` due to rounding, so\n        # trim off any extra pixels at the edges\n        trim_top = ((out.shape[0] - h) // 2)\n        trim_left = ((out.shape[1] - w) // 2)\n        out = out[trim_top:trim_top+h, trim_left:trim_left+w]\n\n    # If zoom_factor == 1, just return the input array\n    else:\n        out = img\n    return out\n\nprint(\"Converting to np.array to list\")\nX_train_augmented = X_train.tolist()\ny_train_augmented = y_train.tolist()\n\n\n# augment data by shifting every image in every direction\nprint(\"Augmenting data by shifting\")\nfor dx, dy in ((5, 0), (-5, 0), (0, 5), (0, -5), (2, 0), (-2, 0), (0, 2), (0, -2)):\n    for image, label in progressbar.progressbar(zip(X_train, y_train), max_value=55000):\n        X_train_augmented.append(shift_image(image, dx, dy))\n        y_train_augmented.append(label)\n\n# augment data by zooming every image in and out\nprint(\"Augmenting data by zooming\")\nfor image, label in progressbar.progressbar(zip(X_train, y_train), max_value=55000):\n    X_train_augmented.append(clipped_zoom(image.reshape(28, 28), 0.7).reshape(-1))\n    y_train_augmented.append(label)\n    X_train_augmented.append(clipped_zoom(image.reshape(28, 28), 1.3).reshape(-1))\n    y_train_augmented.append(label)\n\n# convert to numpy array for imporved speed\nX_train_augmented = np.array(X_train_augmented)\ny_train_augmented = np.array(y_train_augmented)\n\nX_train = X_train_augmented\ny_train = y_train_augmented"},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":"def shuffle_batch(X, y, batch_size):\n    rnd_idx = np.random.permutation(len(X))\n    n_batches = len(X) // batch_size\n    for batch_idx in np.array_split(rnd_idx, n_batches):\n        X_batch, y_batch = X[batch_idx], y[batch_idx]\n        yield X_batch, y_batch"},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"100% (6050 of 6050) |####################| Elapsed Time: 0:03:51 Time:  0:03:51\n0 Last batch accuracy: 0.99 Test accuracy: 0.9902\n100% (6050 of 6050) |####################| Elapsed Time: 0:03:47 Time:  0:03:47\n1 Last batch accuracy: 1.0 Test accuracy: 0.9925\n100% (6050 of 6050) |####################| Elapsed Time: 0:03:59 Time:  0:03:59\n2 Last batch accuracy: 1.0 Test accuracy: 0.9929\n100% (6050 of 6050) |####################| Elapsed Time: 0:03:49 Time:  0:03:49\nN/A% (0 of 6050) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--3 Last batch accuracy: 1.0 Test accuracy: 0.993\n100% (6050 of 6050) |####################| Elapsed Time: 0:03:42 Time:  0:03:42\n4 Last batch accuracy: 1.0 Test accuracy: 0.9932\n"}],"source":"now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\nroot_logdir = \"tf_logs\"\nlogdir = \"{}/run-{}/\".format(root_logdir, now)\n\n# acc_val_summary = tf.summary.scalar('Validation Accuracy', accuracy)\n# file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n\nn_epochs = 5\nbatch_size = 100\nn_batches = len(X_train)//batch_size\n\nwith tf.Session() as sess:\n    init.run()\n    for epoch in range(n_epochs):\n        bar = progressbar.ProgressBar(max_value=n_batches)\n        i = 0\n        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n            bar.update(i)\n            i += 1\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n            if not True:\n                pass\n                summary_str = acc_val_summary.eval(feed_dict={X: X_test, y: y_test})\n                file_writer.add_summary(summary_str, epoch*n_batches+i)\n\n        bar.finish()\n        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n        print(epoch, \"Last batch accuracy:\", acc_batch, \"Test accuracy:\", acc_test)\n\n        save_path = saver.save(sess, \"mnist_cnn_model/my_mnist_model\")\n# file_writer.flush()\n# file_writer.close()"},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[-27.32937   -14.231848  -24.112865   11.604443  -23.256365   29.594835\n","  -11.508104  -26.737427   -2.4051402  -6.2674828]]\n"]}],"source":"with tf.Session() as sess:\n    saver.restore(sess, \"mnist_cnn_model/my_mnist_model\")\n    print(logits.eval(feed_dict={X:X_valid[0].reshape(1, -1), y:y_valid[0].reshape(-1)}))"},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"data":{"text/plain":["0"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":"y_valid[1]"},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":"0"},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":"os.system(\"rundll32.exe powrprof.dll,SetSuspendState\")"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}